{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! rm -rf DonutOCR\n",
        "! git clone https://github.com/PANDASANG1231/DonutOCR.git"
      ],
      "metadata": {
        "id": "XsLW5ppIE4L0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d9cc1b5c-8622-4e4a-ca46-97345c3734c9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'DonutOCR'...\n",
            "remote: Enumerating objects: 32, done.\u001b[K\n",
            "remote: Counting objects: 100% (32/32), done.\u001b[K\n",
            "remote: Compressing objects: 100% (26/26), done.\u001b[K\n",
            "remote: Total 32 (delta 10), reused 28 (delta 6), pack-reused 0 (from 0)\u001b[K\n",
            "Receiving objects: 100% (32/32), 1.61 MiB | 23.96 MiB/s, done.\n",
            "Resolving deltas: 100% (10/10), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.flush_and_unmount()\n",
        "drive.mount('/content/drive',force_remount=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCj0iPv6GEN-",
        "outputId": "d4a4d3c5-7bcd-4e30-ac30-564b77c18496"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import pipeline\n",
        "from PIL import Image\n",
        "\n",
        "image = Image.open(\"drive/MyDrive/th.jpg\")\n",
        "\n",
        "model_pipeline = pipeline(\n",
        "    task=\"document-question-answering\",\n",
        "    model=\"DonutOCR/\",  # Path to your local model folder\n",
        "    device=0 if torch.cuda.is_available() else -1,\n",
        "    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OpWbSkf3VZ6q",
        "outputId": "787335c5-4a65-4ee2-b447-17d4b50d59f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Config of the encoder: <class 'transformers.models.donut.modeling_donut_swin.DonutSwinModel'> is overwritten by shared encoder config: DonutSwinConfig {\n",
            "  \"attention_probs_dropout_prob\": 0.0,\n",
            "  \"depths\": [\n",
            "    2,\n",
            "    2,\n",
            "    14,\n",
            "    2\n",
            "  ],\n",
            "  \"drop_path_rate\": 0.1,\n",
            "  \"embed_dim\": 128,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.0,\n",
            "  \"hidden_size\": 1024,\n",
            "  \"image_size\": [\n",
            "    2560,\n",
            "    1920\n",
            "  ],\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"layer_norm_eps\": 1e-05,\n",
            "  \"mlp_ratio\": 4.0,\n",
            "  \"model_type\": \"donut-swin\",\n",
            "  \"num_channels\": 3,\n",
            "  \"num_heads\": [\n",
            "    4,\n",
            "    8,\n",
            "    16,\n",
            "    32\n",
            "  ],\n",
            "  \"num_layers\": 4,\n",
            "  \"patch_size\": 4,\n",
            "  \"path_norm\": true,\n",
            "  \"qkv_bias\": true,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_absolute_embeddings\": false,\n",
            "  \"window_size\": 10\n",
            "}\n",
            "\n",
            "Config of the decoder: <class 'transformers.models.mbart.modeling_mbart.MBartForCausalLM'> is overwritten by shared decoder config: MBartConfig {\n",
            "  \"activation_dropout\": 0.0,\n",
            "  \"activation_function\": \"gelu\",\n",
            "  \"add_cross_attention\": true,\n",
            "  \"add_final_layer_norm\": true,\n",
            "  \"attention_dropout\": 0.0,\n",
            "  \"bos_token_id\": 0,\n",
            "  \"classifier_dropout\": 0.0,\n",
            "  \"d_model\": 1024,\n",
            "  \"decoder_attention_heads\": 16,\n",
            "  \"decoder_ffn_dim\": 4096,\n",
            "  \"decoder_layerdrop\": 0.0,\n",
            "  \"decoder_layers\": 4,\n",
            "  \"dropout\": 0.1,\n",
            "  \"encoder_attention_heads\": 16,\n",
            "  \"encoder_ffn_dim\": 4096,\n",
            "  \"encoder_layerdrop\": 0.0,\n",
            "  \"encoder_layers\": 12,\n",
            "  \"eos_token_id\": 2,\n",
            "  \"forced_eos_token_id\": 2,\n",
            "  \"init_std\": 0.02,\n",
            "  \"is_decoder\": true,\n",
            "  \"is_encoder_decoder\": false,\n",
            "  \"max_position_embeddings\": 128,\n",
            "  \"model_type\": \"mbart\",\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"pad_token_id\": 1,\n",
            "  \"scale_embedding\": true,\n",
            "  \"torch_dtype\": \"float16\",\n",
            "  \"transformers_version\": \"4.50.3\",\n",
            "  \"use_cache\": true,\n",
            "  \"vocab_size\": 57532\n",
            "}\n",
            "\n",
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\n",
        "  model_pipeline(image=image, question=\"In the top left corner, whho is the issuer?\"),\n",
        "  model_pipeline(image=image, question=\"After to the order of, who is this check pay to?\"),\n",
        "  model_pipeline(image=image, question=\"Follow by date, what date is this check issuing?\"),\n",
        "  model_pipeline(image=image, question=\"In the top right corner, sometimes follow by Check No, what is this check no?\"),\n",
        "  model_pipeline(image=image, question=\"Try to find the amount of money, how much is this check amount?\"),\n",
        "  model_pipeline(image=image, question=\"In the buttom right corner, What name is the signer's name?\")\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cEovb2qwH7ZH",
        "outputId": "0c0582b7-8ef3-4fbc-a8b5-bd8d2fbd2413"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "MBartModel is using MBartSdpaAttention, but `torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True` or `layer_head_mask` not None. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[{'answer': 'michelle cents'}] [{'answer': 'abc water company'}] [{'answer': 'may 1, 2020'}] [{'answer': '5719'}] [{'answer': '$ 110.52'}] [{'answer': 'michelle cents'}]\n"
          ]
        }
      ]
    }
  ]
}